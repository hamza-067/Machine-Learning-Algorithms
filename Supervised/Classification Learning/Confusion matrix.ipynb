{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57979a10-2d56-4ce6-ba3e-6e88df9cfd53",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd8c61-3f38-478c-ac6f-2951cc51cbda",
   "metadata": {},
   "source": [
    "Confusion matrix is a simple table used to measure how well a classification model is performing. It compares the predictions made by the model with the actual results and shows where the model was right or wrong. This helps you understand where the model is making mistakes so you can improve it. It breaks down the predictions into four categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e19d55-0223-46cd-bc41-8321dea4b3ba",
   "metadata": {},
   "source": [
    "True Positive (TP): The model correctly predicted a positive outcome i.e the actual outcome was positive.\n",
    "\n",
    "True Negative (TN): The model correctly predicted a negative outcome i.e the actual outcome was negative.\n",
    "\n",
    "False Positive (FP): The model incorrectly predicted a positive outcome i.e the actual outcome was negative. It is also known as a Type I error.\n",
    "\n",
    "False Negative (FN): The model incorrectly predicted a negative outcome i.e the actual outcome was positive. It is also known as a Type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357ffbb-755f-4eed-a1d5-679c6803eadd",
   "metadata": {},
   "source": [
    "# 1. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20e7e9-27fd-4f2d-a761-116446fac407",
   "metadata": {},
   "source": [
    "Accuracy shows how many predictions the model got right out of all the predictions. It gives idea of overall performance but it can be misleading when one class is more dominant over the other. For example a model that predicts the majority class correctly most of the time might have high accuracy but still fail to capture important details about other classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c781c9b-cdc4-4e21-a882-5645c4fdcb8c",
   "metadata": {},
   "source": [
    "It can be calculated using the below formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba380f-552d-4cc7-a5a9-c34458097997",
   "metadata": {},
   "source": [
    "Accuracy= TP+TN/TP+TN+FP+FN\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2539ea-3826-4b64-8390-c7cbb11ef694",
   "metadata": {},
   "source": [
    "# 2. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392dcd36-2bdf-4a52-b8ef-43dedb326ad3",
   "metadata": {},
   "source": [
    "Precision focus on the quality of the model’s positive predictions. It tells us how many of the \"positive\" predictions were actually correct. It is important in situations where false positives need to be minimized such as detecting spam emails or fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b0f17-a98d-4df9-8f53-c62478a25e5e",
   "metadata": {},
   "source": [
    " The formula of precision is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4c9dd-d56d-4d9d-844d-ed86b46e2750",
   "metadata": {},
   "source": [
    "\n",
    "Precision= TP/TP+FP\n",
    "\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae55ed-8a09-4997-bba2-f3dc24eb51e9",
   "metadata": {},
   "source": [
    "# 3. Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce933083-0548-42e9-8f3d-4a972dfaf24f",
   "metadata": {},
   "source": [
    "Recall measures how how good the model is at predicting positives. It shows the proportion of true positives detected out of all the actual positive instances. High recall is essential when missing positive cases has significant consequences like in medical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cefc0e2-6106-46a3-9ae3-73c12b8c0312",
   "metadata": {},
   "source": [
    "Recall = TP / FP + FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa42d4-5d07-4b9c-9da6-5c637c584ab3",
   "metadata": {},
   "source": [
    "# 4. F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925c0e3-12f7-45bd-90e9-2772fc5da9ba",
   "metadata": {},
   "source": [
    "F1-score combines precision and recall into a single metric to balance their trade-off. It provides a better sense of a model’s overall performance particularly for imbalanced datasets. It is helpful when both false positives and false negatives are important though it assumes precision and recall are equally important but in some situations one might matter more than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b63d3f-5ba3-4a3f-9d9c-eb5f09989ffe",
   "metadata": {},
   "source": [
    "F1-Score = 2⋅Precision⋅Recall / Precision+Recall\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25388ee8-8bde-4978-a993-87f52030b99e",
   "metadata": {},
   "source": [
    "# 5. Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06fb46-40a8-4bee-8793-9a32fe68303d",
   "metadata": {},
   "source": [
    "Specificity is another important metric in the evaluation of classification models particularly in binary classification. It measures the ability of a model to correctly identify negative instances. Specificity is also known as the True Negative Rate Formula is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d4b8e-8172-4752-933a-d16c02f0a2d0",
   "metadata": {},
   "source": [
    "Specificity = TN / TN+FP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18674a03-0136-4da2-9945-2468b9cc9170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
